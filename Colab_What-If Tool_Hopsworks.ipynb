{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Colab_What-If Tool_Hopsworks.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"18c7cb4d"},"source":["---\n","title: \"Colab Hopsworks - Visualization - Feature Analysis with What-If Tool\"\n","date: 2022-01-03\n","type: technical_note\n","draft: true\n","---"]},{"cell_type":"markdown","source":["This notebook shows use of the [What-If Tool](https://pair-code.github.io/what-if-tool) to analyse the performance of a model within Hopsworks inside Google Colab. \n","\n","This notebook trains a linear classifier on the [UCI census problem](https://archive.ics.uci.edu/ml/datasets/census+income) (predicting whether a person earns more than $50K from their census information).\n","\n","It then visualizes the results of the trained classifiers on test data using the What-If Tool.\n","\n","*Original Notebook:* [What-If Tool Notebook Usage](https://colab.research.google.com/github/PAIR-code/what-if-tool/blob/master/What_If_Tool_Notebook_Usage.ipynb)"],"metadata":{"id":"wg6I6KFpdf3b"}},{"cell_type":"code","metadata":{"id":"qqB2tjOMETmr"},"source":["#@title Install the What-If Tool widget if running in colab {display-mode: \"form\"}\n","\n","try:\n","  import google.colab\n","  !pip install --upgrade witwidget\n","except:\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"funky-deviation"},"source":["## Prerequistes\n","\n","### Step 1: Register an account on [hopsworks.ai](https://hopsworks.ai)\n","Click on the \"Demo\" button to access a demo cluster. \n","Copy the URL to the cluster in the form \"[UUID].cloud.hopsworks.ai\". You will need it to connect to Hopsworks later.\n","\n","### Step 2.  Open the Demo Cluster and run the \"Deep Learning Tour\"\n","Note the \"project-name\" that is created when you run the Deep Learning Tour. You will need it to connect to Hopsworks later.\n","\n","### Step 3: Configure a Hopsworks API Key\n","You need to set up a Feature Store API key for authentication.\n","In Hopsworks, click on your username in the top-right corner \n","(1) and select Settings to open the user settings. Select API keys. \n","(2) Give the key a name and select the job, featurestore, dataset.create, kafka and project scopes before \n","(3) creating the key. \n","\n","Copy the key into your clipboard for the next step."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lR7p19HsRdV1","executionInfo":{"status":"ok","timestamp":1642063730845,"user_tz":-60,"elapsed":22899,"user":{"displayName":"Lex Avstreikh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcrewJjVIblvQdxZcmKtXWHNGoNYOKWA1fgwhc=s64","userId":"16586675749120226470"}},"outputId":"244cd5a7-1e63-4f2a-eb10-2fc4709c9e4d"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Skipping hsfs as it is not installed.\u001b[0m\n","Collecting hsfs[hive]\n","  Downloading hsfs-2.4.7.tar.gz (92 kB)\n","\u001b[K     |████████████████████████████████| 92 kB 300 kB/s \n","\u001b[?25hCollecting pyhumps==1.6.1\n","  Downloading pyhumps-1.6.1-py3-none-any.whl (5.0 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from hsfs[hive]) (2.23.0)\n","Collecting furl\n","  Downloading furl-2.1.3-py2.py3-none-any.whl (20 kB)\n","Collecting boto3\n","  Downloading boto3-1.20.34-py3-none-any.whl (131 kB)\n","\u001b[K     |████████████████████████████████| 131 kB 38.5 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from hsfs[hive]) (1.1.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hsfs[hive]) (1.19.5)\n","Collecting pyjks\n","  Downloading pyjks-20.0.0-py2.py3-none-any.whl (45 kB)\n","\u001b[K     |████████████████████████████████| 45 kB 2.2 MB/s \n","\u001b[?25hCollecting mock\n","  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n","Collecting avro==1.10.2\n","  Downloading avro-1.10.2.tar.gz (68 kB)\n","\u001b[K     |████████████████████████████████| 68 kB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from hsfs[hive]) (1.4.29)\n","Collecting PyMySQL\n","  Downloading PyMySQL-1.0.2-py3-none-any.whl (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 1.9 MB/s \n","\u001b[?25hCollecting pyhopshive[thrift]\n","  Downloading PyHopsHive-0.6.4.dev0.tar.gz (42 kB)\n","\u001b[K     |████████████████████████████████| 42 kB 787 kB/s \n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n","  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n","Collecting s3transfer<0.6.0,>=0.5.0\n","  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 5.1 MB/s \n","\u001b[?25hCollecting botocore<1.24.0,>=1.23.34\n","  Downloading botocore-1.23.34-py3-none-any.whl (8.5 MB)\n","\u001b[K     |████████████████████████████████| 8.5 MB 56.0 MB/s \n","\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n","\u001b[K     |████████████████████████████████| 138 kB 67.9 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.34->boto3->hsfs[hive]) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.34->boto3->hsfs[hive]) (1.15.0)\n","Collecting orderedmultidict>=1.0.1\n","  Downloading orderedmultidict-1.0.1-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->hsfs[hive]) (2018.9)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyhopshive[thrift]->hsfs[hive]) (0.16.0)\n","Collecting thrift>=0.10.0\n","  Downloading thrift-0.15.0.tar.gz (59 kB)\n","\u001b[K     |████████████████████████████████| 59 kB 4.9 MB/s \n","\u001b[?25hCollecting twofish\n","  Downloading twofish-0.3.0.tar.gz (26 kB)\n","Collecting javaobj-py3\n","  Downloading javaobj_py3-0.4.3-py2.py3-none-any.whl (57 kB)\n","\u001b[K     |████████████████████████████████| 57 kB 4.1 MB/s \n","\u001b[?25hRequirement already satisfied: pyasn1-modules in /usr/local/lib/python3.7/dist-packages (from pyjks->hsfs[hive]) (0.2.8)\n","Collecting pycryptodomex\n","  Downloading pycryptodomex-3.12.0-cp35-abi3-manylinux2010_x86_64.whl (2.0 MB)\n","\u001b[K     |████████████████████████████████| 2.0 MB 55.9 MB/s \n","\u001b[?25hRequirement already satisfied: pyasn1>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from pyjks->hsfs[hive]) (0.4.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->hsfs[hive]) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->hsfs[hive]) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->hsfs[hive]) (2.10)\n","Collecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 59.3 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->hsfs[hive]) (4.10.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->hsfs[hive]) (1.1.2)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy->hsfs[hive]) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy->hsfs[hive]) (3.7.0)\n","Building wheels for collected packages: hsfs, avro, pyhopshive, thrift, twofish\n","  Building wheel for hsfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hsfs: filename=hsfs-2.4.7-py3-none-any.whl size=146222 sha256=c50a375444572cf816c56e4349e163446de985df7deaa84e2af26587bdf5aced\n","  Stored in directory: /root/.cache/pip/wheels/ae/e0/5d/043b421d3607fc1750c7c957d1d86bbcde95e4e62fb4839f16\n","  Building wheel for avro (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for avro: filename=avro-1.10.2-py3-none-any.whl size=96832 sha256=dbb3ce966524ab31cc959d5c400e0c4bdb79bf9f80040bf5473e6b495eba943b\n","  Stored in directory: /root/.cache/pip/wheels/e7/93/e8/7e16388beb0837cbfb9065ff9d3fe33e4111a3f4bedea1c2c6\n","  Building wheel for pyhopshive (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyhopshive: filename=PyHopsHive-0.6.4.dev0-py3-none-any.whl size=48452 sha256=a662cc90b8531c5ffa5e17812b64ff6c236a398ee32825f88774e6ff757f1393\n","  Stored in directory: /root/.cache/pip/wheels/fd/09/36/c63472c49e18d18ab14df3b17abb54b6aba60756c7030911e8\n","  Building wheel for thrift (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for thrift: filename=thrift-0.15.0-cp37-cp37m-linux_x86_64.whl size=348210 sha256=93b5ef20db9d8bee460bb0320d179399028276b8949c63f1c4894ee795222d04\n","  Stored in directory: /root/.cache/pip/wheels/ba/1f/8e/e6fd36837eecf3d1f2b23f1729477e8e06558d8d60b7093f51\n","  Building wheel for twofish (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for twofish: filename=twofish-0.3.0-cp37-cp37m-linux_x86_64.whl size=28154 sha256=1f76b5b6b143f0822328472ed36c345e37479ccd05c49fc18e0ceb119a9efe2d\n","  Stored in directory: /root/.cache/pip/wheels/ae/fd/45/5f33a6da0bf1f7643ed2b7f4ef124d6fe049d2b1604ccdf83d\n","Successfully built hsfs avro pyhopshive thrift twofish\n","Installing collected packages: urllib3, jmespath, twofish, pycryptodomex, javaobj-py3, botocore, s3transfer, pyjks, orderedmultidict, thrift, PyMySQL, pyhumps, pyhopshive, mock, furl, boto3, avro, hsfs\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed PyMySQL-1.0.2 avro-1.10.2 boto3-1.20.34 botocore-1.23.34 furl-2.1.3 hsfs-2.4.7 javaobj-py3-0.4.3 jmespath-0.10.0 mock-4.0.3 orderedmultidict-1.0.1 pycryptodomex-3.12.0 pyhopshive-0.6.4.dev0 pyhumps-1.6.1 pyjks-20.0.0 s3transfer-0.5.0 thrift-0.15.0 twofish-0.3.0 urllib3-1.25.11\n"]}],"source":["!pip3 uninstall hsfs -y\n","!pip3 install hsfs[hive] --no-warn-conflicts"]},{"cell_type":"code","source":["import hsfs\n","\n","# TODO: replace the values below: [UUID], [project-name], [api-key]\n","connection = hsfs.connection(host=\"[UUID].cloud.hopsworks.ai\",   # UUID is from Step 1, above\n","    project=\"[project-name]\",                                    # project-name is from Step 2, above\n","    engine=\"hive\",\n","    api_key_value=\"[api-key]\")                                   # the API key comes from Step 3, above\n","\n","fs = connection.get_feature_store()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aRqS1djMRkeC","executionInfo":{"status":"ok","timestamp":1639391145849,"user_tz":-60,"elapsed":4458,"user":{"displayName":"Anastasiia Andriievska","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09301832849769378230"}},"outputId":"7e200e2d-afab-4d40-acf5-c249b5937928"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Connected. Call `.close()` to terminate connection gracefully.\n"]}]},{"cell_type":"markdown","metadata":{"id":"4073756a"},"source":["Run the \"Deep Learning Tour\" in Hopsworks to create the demo Machine Learning project.\n"]},{"cell_type":"code","metadata":{"id":"jlwjF-Nnmoww"},"source":["#@title Define helper functions {display-mode: \"form\"}\n","\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import functools\n","\n","# Creates a tf feature spec from the dataframe and columns specified.\n","def create_feature_spec(df, columns=None):\n","    feature_spec = {}\n","    if columns == None:\n","        columns = df.columns.values.tolist()\n","    for f in columns:\n","        if df[f].dtype is np.dtype(np.int64):\n","            feature_spec[f] = tf.io.FixedLenFeature(shape=(), dtype=tf.int64)\n","        elif df[f].dtype is np.dtype(np.float64):\n","            feature_spec[f] = tf.io.FixedLenFeature(shape=(), dtype=tf.float32)\n","        else:\n","            feature_spec[f] = tf.io.FixedLenFeature(shape=(), dtype=tf.string)\n","    return feature_spec\n","\n","# Creates simple numeric and categorical feature columns from a feature spec and a\n","# list of columns from that spec to use.\n","#\n","# NOTE: Models might perform better with some feature engineering such as bucketed\n","# numeric columns and hash-bucket/embedding columns for categorical features.\n","def create_feature_columns(columns, feature_spec):\n","    ret = []\n","    for col in columns:\n","        if feature_spec[col].dtype is tf.int64 or feature_spec[col].dtype is tf.float32:\n","            ret.append(tf.feature_column.numeric_column(col))\n","        else:\n","            ret.append(tf.feature_column.indicator_column(\n","                tf.feature_column.categorical_column_with_vocabulary_list(col, list(df[col].unique()))))\n","    return ret\n","\n","# An input function for providing input to a model from tf.Examples\n","def tfexamples_input_fn(examples, feature_spec, label, mode=tf.estimator.ModeKeys.EVAL,\n","                       num_epochs=None, \n","                       batch_size=64):\n","    def ex_generator():\n","        for i in range(len(examples)):\n","            yield examples[i].SerializeToString()\n","    dataset = tf.data.Dataset.from_generator(\n","      ex_generator, tf.dtypes.string, tf.TensorShape([]))\n","    if mode == tf.estimator.ModeKeys.TRAIN:\n","        dataset = dataset.shuffle(buffer_size=2 * batch_size + 1)\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(lambda tf_example: parse_tf_example(tf_example, label, feature_spec))\n","    dataset = dataset.repeat(num_epochs)\n","    return dataset\n","\n","# Parses Tf.Example protos into features for the input function.\n","def parse_tf_example(example_proto, label, feature_spec):\n","    parsed_features = tf.io.parse_example(serialized=example_proto, features=feature_spec)\n","    target = parsed_features.pop(label)\n","    return parsed_features, target\n","\n","# Converts a dataframe into a list of tf.Example protos.\n","def df_to_examples(df, columns=None):\n","    examples = []\n","    if columns == None:\n","        columns = df.columns.values.tolist()\n","    for index, row in df.iterrows():\n","        example = tf.train.Example()\n","        for col in columns:\n","            if df[col].dtype is np.dtype(np.int64):\n","                example.features.feature[col].int64_list.value.append(int(row[col]))\n","            elif df[col].dtype is np.dtype(np.float64):\n","                example.features.feature[col].float_list.value.append(row[col])\n","            elif row[col] == row[col]:\n","                example.features.feature[col].bytes_list.value.append(row[col].encode('utf-8'))\n","        examples.append(example)\n","    return examples\n","\n","# Converts a dataframe column into a column of 0's and 1's based on the provided test.\n","# Used to force label columns to be numeric for binary classification using a TF estimator.\n","def make_label_column_numeric(df, label_column, test):\n","  df[label_column] = np.where(test(df[label_column]), 1, 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nu398ARdeuxe"},"source":["#@title Read training dataset from CSV {display-mode: \"form\"}\n","\n","import pandas as pd\n","\n","# Set the path to the CSV containing the dataset to train on.\n","csv_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n","\n","# Set the column names for the columns in the CSV. If the CSV's first line is a header line containing\n","# the column names, then set this to None.\n","csv_columns = [\n","  \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital-Status\",\n","  \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital-Gain\", \"Capital-Loss\",\n","  \"Hours-per-week\", \"Country\", \"Over-50K\"]\n","\n","# Read the dataset from the provided CSV and print out information about it.\n","df = pd.read_csv(csv_path, names=csv_columns, skipinitialspace=True)\n","\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"67DYIFxoevt2"},"source":["#@title Specify input columns and column to predict {display-mode: \"form\"}\n","import numpy as np\n","\n","# Set the column in the dataset you wish for the model to predict\n","label_column = 'Over-50K'\n","\n","# Make the label column numeric (0 and 1), for use in our model.\n","# In this case, examples with a target value of '>50K' are considered to be in\n","# the '1' (positive) class and all other examples are considered to be in the\n","# '0' (negative) class.\n","make_label_column_numeric(df, label_column, lambda val: val == '>50K')\n","\n","# Set list of all columns from the dataset we will use for model input.\n","input_features = [\n","  'Age', 'Workclass', 'Education', 'Marital-Status', 'Occupation',\n","  'Relationship', 'Race', 'Sex', 'Capital-Gain', 'Capital-Loss',\n","  'Hours-per-week', 'Country']\n","\n","# Create a list containing all input features and the label column\n","features_and_labels = input_features + [label_column]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BV4f_4_Lex22"},"source":["#@title Convert dataset to tf.Example protos {display-mode: \"form\"}\n","\n","examples = df_to_examples(df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YyLr-_0de1Ii"},"source":["#@title Create and train the linear classifier {display-mode: \"form\"}\n","\n","num_steps = 1000  #@param {type: \"number\"}\n","\n","# Create a feature spec for the classifier\n","feature_spec = create_feature_spec(df, features_and_labels)\n","\n","# Define and train the classifier\n","train_inpf = functools.partial(tfexamples_input_fn, examples, feature_spec, label_column)\n","classifier = tf.estimator.LinearClassifier(\n","    feature_columns=create_feature_columns(input_features, feature_spec))\n","classifier.train(train_inpf, steps=num_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NUQVro76e38Q"},"source":["#@title Invoke What-If Tool for test data and the trained models {display-mode: \"form\"}\n","\n","num_datapoints = 2000  #@param {type: \"number\"}\n","tool_height_in_px = 1000  #@param {type: \"number\"}\n","\n","from witwidget.notebook.visualization import WitConfigBuilder\n","from witwidget.notebook.visualization import WitWidget\n","\n","# Load up the test dataset\n","test_csv_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'\n","test_df = pd.read_csv(test_csv_path, names=csv_columns, skipinitialspace=True,\n","  skiprows=1)\n","make_label_column_numeric(test_df, label_column, lambda val: val == '>50K.')\n","test_examples = df_to_examples(test_df[0:num_datapoints])\n","\n","# Setup the tool with the test examples and the trained classifier\n","config_builder = WitConfigBuilder(test_examples).set_estimator_and_feature_spec(\n","    classifier, feature_spec).set_label_vocab(['Under 50K', 'Over 50K'])\n","WitWidget(config_builder, height=tool_height_in_px)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"8GRKq0JB3Vzz"},"execution_count":null,"outputs":[]}]}